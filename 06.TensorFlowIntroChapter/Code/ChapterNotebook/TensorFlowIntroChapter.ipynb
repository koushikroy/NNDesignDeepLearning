{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NNDesignDeepLearning/NNDesignDeepLearning/blob/master/06.TensorFlowIntroChapter/Code/ChapterNotebook/TensorFlowIntroChapter.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This chapter provides an introduction to the TensorFlow framework for deep learning. There are different interfaces that can be used to access TensorFlow. In this chapter we begin with the simplest one. We will show how you can quickly get started to perform all aspects of the typical deep learning workflow: load and preprocess data, build a neural network, train the network and evaluate the results.\n",
    "\n",
    "\n",
    "# Theory and Examples\n",
    "\n",
    "\n",
    "**TensorFlow** is one of the most popular deep learning frameworks. It was originally developed by the Google Brain group starting in 2011 as DistBelief. It was further developed into TensorFlow and released as open source software in November of 2015. TensorFlow 2.0 became officially available in Sep 2019. It is written in Python and C++.\n",
    "\n",
    "There is more than one API (Application Programming Interface) for TensorFlow. We will mainly focus on the **Keras** API in this introduction. We will cover other APIs in Chapter 11.\n",
    "\n",
    "Keras was originally developed independently from TensorFlow. According to its developer, Fran&ccedil;ois Chollet, Keras was more of an interface than a framework. This is because Keras was originally designed to act as a frontend for some of the other frameworks. In particular, the TensorFlow, Theano and CNTK frameworks can currently be backends for Keras, and others may be implemented in the future.\n",
    "\n",
    "Keras was developed during 2014 and 2015, as part of a research project with ONEIROS. It started as an interface to Theano (a predecessor framework to TensorFlow) that enabled the use of recurrent neural networks (RNNs) and convolution neural networks. Its first release was in March of 2015. Fran&ccedil;ois Chollet later joined Google, and in 2017 the Google TensorFlow team decided to support Keras in TensorFlow's core library. For TenorFlow 2.0, released in 2019, Keras is the central API for TensorFlow.\n",
    "\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "The first step in using any framework is to load, format and preprocess the data set. For Keras, the data can be loaded using NumPy arrays (tensors). For other APIs or frameworks the data can be conveniently converted from a NumPy array to a similar tensor format before training. In each case, the important thing to know is what each axis of the tensor represents.\n",
    "\n",
    "For standard multilayer networks the input is a vector with `R` elements, which we often refer to as features. The training data set will have `Q` samples of the input vector. For Keras this can be stored as a `(Q, R)`, or (samples, features), tensor.  \n",
    "\n",
    "For other types of networks the input data can be a time series. In this case, the Keras input will be a 3D tensor of the form (samples, timesteps, features). The term **samples** here indicates the number of different time series in the data set, each of which would be of length *timesteps*.\n",
    "\n",
    "Some networks can also accommodate images, which usually have three dimensions: height, width and color. The 4D input tensors for the network have the form (samples, height, width, color). The color (also called *channel*) dimension is usually three (red, green and blue), but can be one for grayscale images or greater than three for multi-spectral images. (It is possible to have the channel axis come before the height and width axes, but channel-last is the default, and it is the format used by TensorFlow.)\n",
    "\n",
    "It is also possible for network inputs to be a time series of images -- a video. The 5D input tensors in this case have the form (samples, timesteps, height, width, channels).\n",
    "\n",
    "In Keras, the network outputs will also be NumPy tensors, which should match the targets. The dimensions for the targets are assigned in the same order as the input dimensions, although the shape of the target is not usually the same as the shape of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR \n",
    "For simplicity, we begin by generating data for the XOR problem. This is a binary classification problem with two dimensional inputs. If the two elements of the input vector are equal, the input is from class 1, and if the two elements are different, the input is from class 2. Here we define the inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "p = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "t = np.array([0, 1, 1, 0])\n",
    "t = to_categorical(t)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to_categorical` function is convenient for creating a one hot encoding of the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "The core data structure in Keras is the **model**. The model is a way to organize layers of a network. There are three ways to define a model: 1) the sequential class, 2) the functional API, and 3) the model subclass.\n",
    "\n",
    "To use the sequential class (which is used for networks where each layer follows the previous one), you begin by creating an instance of the class, and then use the method `add` to add layers, as in the following example. Here we create a two-layer network with 10 neurons in the hidden layer and a `tanh` activation function. The network architecture is 2-10-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10, activation='tanh', input_shape=(2,)))\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for the first layer we need to assign the input size. For the following layers, the input size can be determined from the size of the previous layers. The final layer uses two `softmax` neurons, which determine two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **API** (Application Programming Interface) is a set of methods for communicating between software components. In this case, Keras is communicating with TensorFlow and cuda libraries. There are often several APIs that can be used. For the **Functional API**, the layers of a network are defined individually with their specific inputs and outputs. For example, the following code implements the same 2-10-2 network using the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = layers.Input(shape=(2,))\n",
    "a1 = layers.Dense(10, activation='tanh')(p1)\n",
    "a2 = layers.Dense(2, activation='softmax')(a1)\n",
    "model = models.Model(inputs=p1, outputs=a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use the **model subclass** method, you create your own fully-customizable models by subclassing the `Model` class and implementing your own forward pass in the `call` method. The following code implements the same 2-10-2 network using the model subclass method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twolayer(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Twolayer,self).__init__()\n",
    "        self.dense1 = layers.Dense(10, activation='tanh')\n",
    "        self.dense2 = layers.Dense(2, activation='softmax')\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        output = self.dense2(x)\n",
    "        return output\n",
    "\n",
    "model = Twolayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the input will not be defined until the model is trained with the `fit` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "After the data has been loaded and the network has been created, the next step is to train the network. In the following we will cover the basic training steps.\n",
    "\n",
    "Before training a network, the model needs to be *compiled*. During compilation you assign the training algorithm and performance (loss) function. In the following example we use the Adam training algorithm and the cross entropy performance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the network using the `fit` method. We supply the inputs and the targets and specify the number of *epochs* for training. An epoch refers to a section of the training process during which the entire training data set is used. A related term is *iteration*, which refers to the updating of the network weights and biases. The weights can be updated in three ways: 1) one sample at a time (as in the stochastic gradient descent algorithm), 2) on the full training set as a whole (batch training) or 3) on minibatches of the training set.  If full batch training is used, then the number of iterations is the same as the number of epochs. However, if minibatches are used, the number of iterations will be larger than the number of epochs. In the example below, we set the number of epochs to 5000 and the batch size to 4. Since the batch size is the same as the size of the full data set, the number of iterations will equal the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(p,t,epochs=5000,batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `history` contains information about the progress of the training process. After training is complete, we can plot the progress of the loss function during training.\n",
    "\n",
    "The `history.history` object is a dictionary that contains a history of the loss function, and `history.epoch` contains a list of the epoch numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "epochs = history.epoch\n",
    "plt.semilogy(epochs, loss_values)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method for the trained model can be used to calculate the network response to an arbitrary set of inputs. Here we apply the training inputs to the model to check the response. Is the response correct for the **XOR** problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example code does not indicate whether the program will execute on a GPU or a CPU. In TensorFlow, the program will automatically run on a GPU, if one is available. If you specifically want some code to run on a CPU, even though a GPU is available, you can manually do that using the following line: \n",
    "\n",
    "`with tf.device('/cpu:0')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Loading\n",
    "\n",
    "Loading/formatting/pre-processing data is one of the most important parts of the deep learning workflow. Sometimes this process is referred to as *Exract, Transform and Load* (**ETL**). First, the data are taken from one or multiple files, which may be distributed across multiple machines. Next, the data is transformed. This process can be as simple as normalizing the data, or may involve augmenting data by rotating or scaling images, adding noise, etc. Finally, the data is loaded into the training process, often in minibatches. \n",
    "\n",
    "Unfortunately, this ETL process can vary significantly from one application to another, so it is difficult to cover all the options. We will give detailed examples for several specific applications in our case study chapters. However, let's take a quick look at some of the advanced data loading concepts.\n",
    "\n",
    "For more sophisticated data loading, you can make the first argument to the `fit` method a **data generator**.  The data generator can be a Python generator, which we described in the Python chapter, or an instance of the `tensorflow.keras.utils.Sequence` class. This class of object is designed for loading data sets. It is expected to have at least two methods: `__get_item__`, which returns the next batch of data, and `__len__`, which returns the number of examples in the data set. It can also have an `on_epoch_end` method, if the data set needs to be modified in some way at each epoch (e.g., by shuffling the data). \n",
    "\n",
    "There are a number of reasons for using a data generator, instead of passing the entire training set into the `fit` method as a NumPy array. First, the data may be too large to fit into memory, so they may need to be loaded one minibatch at a time. Also, you may want to distribute the computation across multiple processors, which can be done conveniently using the `tensorflow.keras.utils.Sequence` class. In addition, a data loader can modify the data during training (e.g., shuffle data or augment data using transformations or noise).\n",
    "\n",
    "\n",
    "### TensorFlow Dataset\n",
    "\n",
    "TensorFlow has a very useful API for creating input pipelines: `tf.data.Dataset`. These pipelines can also be passed to the `fit` method instead of a data generator. There are many ways to use `tf.data.Dataset`, and we will cover several of these in Chapter 11 and in the Case Study chapters. To give an idea of how `tf.data.Dataset` can be used, let's consider one method from the API: `from_tensor_slices`. This method can create a Dataset from a NumPy array, or other types of data structures, like lists and dictionaries. To illustrate the operation, we'll work with the CSV file that we used in the Python chapter and lab.\n",
    "\n",
    "First, we read in CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/NNDesignDeepLearning/NNDesignDeepLearning/master/05.PythonChapter/Code/ChapterNotebook/SampleDF.csv?token=AUZTTDQALCLLAYUY2ZZGDKDA5YTEK'\n",
    "sample_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract two columns that we will use as inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array(sample_df['FVC'])\n",
    "T = np.array(sample_df['Percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data into a Dataset, using `from_tensor_slices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_tensor_slices((P, T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Dataset can then be passed to the `fit` method. The Dataset is an iterable, like a data generator, so we can also access the elements with a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, targ in dataset.take(5):\n",
    "  print ('Features: {}, Target: {}'.format(feat, targ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data are loaded into the Dataset, there are many useful operations that can be efficiently performed. With the `batch` method, for example, we can group the Dataset into minibatches. In the following example we group the Dataset into minibatches of size 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, targ in dataset.take(5):\n",
    "  print ('Features: {}, Target: {}'.format(feat, targ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 11 and the case study chapters we will investigate other features of the Dataset that allow us to distribute operations across multiple gpus, incorporate augmentation into the data pipeline, prefetch data, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
